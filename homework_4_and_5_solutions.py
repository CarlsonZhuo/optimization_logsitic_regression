# -*- coding: utf-8 -*-
from __future__ import division
"""Homework 4 and 5 Solutions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wy5pb-hiyj_pkGf4rh--oZMwbKdUlqXo

# Problem Set 4
In this problem set you will get some practice with the proximal gradient algorithm, and also acceleration. Specifically, you will be implementing ISTA and FISTA

# Problem 1: Gradient Descent and Acceleration
In this problem you will explore the impact of ill-conditioning on gradient descent, and will then see how acceleration can improve the situation. This exercise will walk you through a very similar situation as to what we saw in the lecture videos that illustrate the performance of gradient descent vs accelerated gradient descent as the condition number (ratio of largest to smallest eigenvalues of the Hessian) increases. This is a ``toy'' problem, but it is still instructive regarding the performance of these two algorithms.

You will work with the following simple function:
$$
f(x) = \frac{1}{2}x^{\top}Qx,
$$
where $Q$ is a 2 by 2 matrix, as defined below.
"""

# We create the data for this simple problem. We will create three quadratics.
# Q_wc -- this is a well-conditioned matrix
# Q_ic -- this is an ill-conditioned matrix
# Q_sic -- this is... a somewhat-ill-conditioned matrix (a technical term!)
import numpy as np
Q_wc = np.array([[1,0.3],[0.3,1]]); q = np.array([0,0]);
Q_sic = np.array([[1,0.85],[0.85,1]]); q = np.array([0,0]);
Q_ic = np.array([[1,0.99],[0.99,1]]); q = np.array([0,0]);

"""# Part (A): 
Consider the quadratic functions $f_{wc}$, $f_{sic}$, and $f_{ic}$ defined by the quadratic matrices above. For each of these, say whether they are $\beta$-smooth and/or $\alpha$-strongly convex, and if so, compute the value of the condition number, $\kappa = \beta/\alpha$ for each function.

# Part (B): 
Compute the best fixed step size for gradient descent, and the best parameters for accelerated gradient descent. For each function, plot the error $(f(x_t) - f(x^{\ast})$ as a function of the number of iterations. For each function, plot these on the same plot so you can compare -- so you should have 3 plots total.

## Problem 2: ISTA and FISTA
Recall the least squares problem with $\ell^1$ regularization from the previous homework:
$$
\min_x \left[f(x) = \frac{1}{2}\|{Ax-b}\|_2^2 + \lambda \|{x}\|_1 \right]
$$

Recall key characteristics of this problem: it is nonsmooth due to the regularization term, and it is not strongly convex when $A$ has more columns than rows. This is why you used the sub-gradient method on the previous problem set, rather than Gradient descent. 

Recall the goal of the proximal gradient algorithm: when we have a composite function, i.e., a function of the form $f(x) = g(x) + h(x)$, if $g(x)$ is $\beta$-smooth and $h(x)$ is ``simple'' in the sense that it has a simple prox function, then rather than using the subgradient method, we can get much better results by using proximal gradient, which takes advantage of the fact that $g(x)$ is smooth. We can improve this further by combining the proximal gradient method with acceleration. 

Using the same data (same $A$ and $b$) as in Problem Set 3, minimize $f(x)$ using $10^4$ iterations with $t=0$ and $x_0 =0$. 

Use the proximal gradient algorithm, also known as ISTA for the case where $f$ is the LASSO objective. Now use the accelerated proximal gradient algorithm, also, known as FISTA. Plot these results on the same plot as your results for sub-gradient descent from the previous lecture.
"""

import numpy as np
import numpy.random as rn
import numpy.linalg as la
import matplotlib.pyplot as plt
import csv
import time
import zipfile as zipfile
import pdb

from sklearn.datasets import fetch_20newsgroups
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF

def descent(algo,func,grad,hess,train,test,params,linesearch):
    var=params.initvar
    numtr=train.X.shape[0]
    numte=test.X.shape[0]
    loss,reg,te_error,elapsed_time=[],[],[],[]
    start=time.time()
    for it in range(params.max_iter):
        var = algo(var,train,func,grad,hess,linesearch,it,params)
        end=time.time()
        beta,gamma = var.beta,var.gamma        
        loss.append(func(gamma,train,params,numtr)[0])
        reg.append(func(gamma,train,params,numtr)[1])
        te_error.append(func(gamma,test,params,numte)[0])
        elapsed_time.append(end-start)
    return beta,loss,reg,te_error,elapsed_time

def plotresult(loss,algo,params,marker,title,ylabel=' ',xlabel=' ',yscale='linear',isSave=False,filename=None,ylim=None,xlim=None):
    fig=plt.figure()
    for i in range(len(loss)):
        plt.plot(loss[i], label=algo[i],marker=marker[i],markevery=int((params.max_iter)/10))
    plt.title(title)
    plt.yscale(yscale)
    plt.legend()
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.xlim(xlim)
    plt.ylim(ylim)
    if isSave:
        plt.savefig(filename)
    plt.show()

def loaddata(filename):
    import io
    data=[]
    with zipfile.ZipFile(filename) as z:
        for filename in z.namelist():
            if filename[0]=='X'or filename[0]=='y':
                each=[]
                with z.open(filename) as csvDataFile:
                    # csvReader=csv.reader(csvDataFile)
                    csvDataFile_utf8 = io.TextIOWrapper(csvDataFile, 'utf-8')
                    csvReader=csv.reader(csvDataFile_utf8)
                    for row in csvReader:
                        each.append(row)
                    each==[[float(string) for string in row] for row in each]
                    each=np.asarray(each)
                    data.append(each)    
    X_te=data[0].astype(float)
    X_tr=data[1].astype(float)
    y_te=data[2][0].astype(int)
    y_tr=data[3][0].astype(int)
    Z_tr,Z_te=[],[]
    for j in range(len(np.unique(y_tr))):
        Z_tr.append(np.sum(X_tr[np.where(y_tr==j)[0],:],axis=0))
        Z_te.append(np.sum(X_te[np.where(y_te==j)[0],:],axis=0))
    Z_tr=np.asarray(Z_tr).T
    Z_te=np.asarray(Z_te).T
    train= Data(X_tr,y_tr)
    train.Z=Z_tr
    test = Data(X_te,y_te)
    test.Z=Z_te
    return train,test

def euclidean_proj_simplex(v, s=1):
#     Referenced by https://gist.github.com/daien/1272551 based on
#     "Efficient Projections onto the .1-Ball for Learning in High Dimensions
#         John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra.
#         International Conference on Machine Learning (ICML 2008)"
#         http://www.cs.berkeley.edu/~jduchi/projects/DuchiSiShCh08.pdf
    assert s > 0, "Radius s must be strictly positive (%d <= 0)" % s
    n, = v.shape  
    if v.sum() == s and np.alltrue(v >= 0):
        return v
    u = np.sort(v)[::-1]
    cssv = np.cumsum(u)
    rho = np.nonzero(u * np.arange(1, n+1) > (cssv - s))[0][-1]
    theta = (cssv[rho] - s) / (rho + 1.0)
    w = (v - theta).clip(min=0)
    return w

# algorithms
def frankwolfe(var,data,func,grad,hess,linesearch,iter,params):
    numpts=params.numpts
    R=params.R
    c=params.stepsize
    beta=var.beta
    g=grad(beta,data,params,numpts)[0]
    desc=-g
    idx=np.argmax(np.abs(g))    
    gamma=np.zeros(data.X.shape[1])
    gamma[idx]=-R * np.sign(g[idx])
    eta=c*linesearch(beta,func,grad,desc,data,params,iter)
    beta=(1-eta)*beta+eta*gamma
    var=Variable(beta,beta,0)
    return var

def ista(var,data,func,grad,hess,linesearch,iter,params):
    numpts=params.numpts
    beta=var.beta
    reg=params.reg
    desc=-grad(beta,data,params,numpts)[0]
    eta=params.stepsize
    beta=beta+eta*desc
    beta=np.sign(beta)*np.maximum(abs(beta)-reg*eta,0)
    var=Variable(beta,beta,0)
    return var

def fista(var,data,func,grad,hess,linesearch,iter,params):
    numpts=params.numpts
    beta,gamma,tau=var.beta,var.gamma,var.tau
    reg=params.reg
    tau_old=tau
    tau=(1+np.sqrt(1+4*np.square(tau)))/2
    ratio=(1-tau_old)/tau
    beta_old=beta
    gamma_old=gamma
    desc=-grad(beta,data,params,numpts)[0]
    eta=params.stepsize
    gamma=beta+eta*desc
    gamma=np.sign(gamma)*np.maximum(abs(gamma)-reg*eta,0)
    beta=(1-ratio)*gamma+ratio*(gamma_old)
    var=Variable(beta,gamma,tau)
    return var

def gd(var,data,func,grad,hess,linesearch,iter,params):
    numpts=params.numpts
    beta=var.beta
    desc=-np.sum(grad(beta,data,params,numpts),axis=0)
    eta=linesearch(beta,func,grad,desc,data,params,iter)
    beta=beta+eta*desc
    var=Variable(beta,beta,0)
    return var
def pgd(var,data,func,grad,hess,linesearch,iter,params):
    numpts=params.numpts
    beta=var.beta
    desc=-np.sum(grad(beta,data,params,numpts),axis=0)
    eta=linesearch(beta,func,grad,desc,data,params,iter)
    beta=beta+eta*desc
    beta=euclidean_proj_simplex(beta)
    var=Variable(beta,beta,0)
    return var

def agd(var,data,func,grad,hess,linesearch,iter,params):
    numpts=params.numpts
    beta,gamma,tau=var.beta,var.gamma,var.tau
    tau_old=tau
    tau=(1+np.sqrt(1+4*np.square(tau)))/2
    ratio=(1-tau_old)/tau
    beta_old=beta
    gamma_old=gamma
    desc=-np.sum(grad(beta,data,params,numpts),axis=0)
    eta=linesearch(beta,func,grad,desc,data,params,iter)
    gamma=beta+eta*desc
    beta=(1-ratio)*gamma+ratio*(gamma_old)    
    var=Variable(beta,gamma,tau)
    return var
def magd(var,data,func,grad,hess,linesearch,iter,params):
    numpts=params.numpts
    beta,gamma,tau=var.beta,var.gamma,var.tau
    tau_old=tau
    tau=(1+np.sqrt(1+4*np.square(tau)))/2
    ratio=(1-tau_old)/tau
    beta_old=beta
    gamma_old=gamma
    desc=-np.sum(grad(beta,data,params,numpts),axis=0)
    eta=linesearch(beta,func,grad,desc,data,params,iter)
    gamma=beta+eta*desc
    beta=(1-ratio)*gamma+ratio*(gamma_old)
    if np.sum(func(beta,data,params,numpts)) > np.sum(func(beta_old,data,params,numpts)):
        beta=beta_old
    var=Variable(beta,gamma,tau)
    return var
def mirrordescent(var,data,func,grad,hess,linesearch,iter,params):
    numpts=params.numpts
    beta=var.beta
    desc=-np.sum(grad(beta,data,params,numpts),axis=0)
    eta=linesearch(beta,func,grad,desc,data,params,iter)
    gamma=beta*np.exp(eta*desc)
    beta=euclidean_proj_simplex(gamma)
    var=Variable(beta,beta,0)
    return var

#linesearch
def btls(beta,func,grad,desc,data,params,iter):
    a,b=params.a,params.b
    ndata=data.X.shape[0]
    t=1
    while np.sum(func(beta+t*desc,data,params,ndata))>np.sum(func(beta,data,params,ndata))+a*t*np.dot(np.sum(grad(beta,data,params,ndata),axis=0).T,desc):
        t=b*t
    return t
def constantstep(beta,func,grad,desc,data,params,iter):
    return params.stepsize
def fixedstep1(beta,func,grad,desc,data,params,iter):
    return params.stepsize/(iter+1)
def fixedstep2(beta,func,grad,desc,data,params,iter):
    return params.stepsize/np.sqrt(iter+1)

# classes
class Parameter:
    def __init__(self,initbeta,initvar):
        self.initbeta=initbeta
        self.initvar=initvar
class Data:
    def __init__(self,X,Y):
        self.X=X
        self.Y=Y        
class Variable:
    def __init__(self,beta,gamma,tau):
        self.beta=beta
        self.gamma=gamma
        self.tau=tau
        
# functions and gradients
# Linear regression
def func_linreg(beta,data,params,numpts):
    n,d=data.X.shape
    if numpts==n:
        X,Y=data.X,data.Y
        loss=(1./Y.shape[0])*(.5)*np.sum((np.dot(X,beta)-Y)**2)
        reg=params.reg*la.norm(beta,1)
        return loss,reg        
    elif numpts==1:
        idx=params.idx
        X,Y=data.X[idx,:],data.Y[idx]
        loss=.5*(np.dot(X,beta)-Y)**2
        reg=params.reg*abs(beta)
        return loss,reg        
    else:
        print('error')   
def grad_linreg(beta,data,params,numpts):
    n,d=data.X.shape
    if numpts==n:
        X,Y=data.X,data.Y
        loss=(1./Y.shape[0])*np.dot(X.T,np.dot(X,beta)-Y)
        reg=params.reg*np.sign(beta)
        return loss,reg
    elif numpts==1:
        idx=params.idx
        X,Y=data.X[idx,:],data.Y[idx]
        loss=(np.dot(X,beta)-Y)*X
        reg=params.reg*np.sign(beta)
        return loss,reg
    else:
        print('error')

# logistic egression with two classes
def func_logreg(beta,data,params,numpts):
    n,d=data.X.shape
    if numpts==n:
        X,Y=data.X,data.Y
        loss=(1./Y.shape[0])*np.sum(np.log(1.+np.exp(np.dot(X,beta)*(-Y))))
        reg=params.reg*np.sum(beta**2)
        return loss,reg
    elif numpts==1:
        idx=params.idx
        X,Y=data.X[idx,:],data.Y[idx]
        loss=np.log(1.+np.exp(np.dot(X,beta)*(-Y)))
        reg=params.reg*np.sum(beta**2)
        return loss,reg
    else:
        print('error')
        
def grad_logreg(beta,data,params,numpts):
    n,d=data.X.shape
    if numpts==n:
        X,Y=data.X,data.Y        
        g_loss=(1./Y.shape[0])*np.dot(X.T,-Y*(1./(1.+np.exp(np.dot(X,beta)*Y))))
        g_reg=2*params.reg*beta
        return g_loss,g_reg
    elif numpts==1:
        idx=params.idx
        X,Y=data.X[idx,:],data.Y[idx]
        g_loss=X.T*(-Y*(1./(1.+np.exp(np.dot(X,beta)*Y))))   
        g_reg=2*params.reg*beta
        return g_loss,g_reg
    else:
        print('error')
        
        
# logistic Regression with multi classes
def func_logreg_multiclass(beta,data,params,numpts):
    n,d=data.X.shape
    if numpts==n:
        X,Y,Z=data.X,data.Y,data.Z
        loss=(1./Y.shape[0])*(np.trace(np.dot(beta.T,Z))+np.sum(np.log(np.sum(np.exp(-np.dot(X,beta)),axis=1))))
        reg=params.reg*la.norm(beta,'fro')**2
        return loss,reg
    elif numpts==1:
        idx=params.idx
        X,Y=data.X[idx,:],data.Y[idx]
        loss=np.dot(beta[:,Y],X)+np.log(np.sum(np.exp(-np.dot(beta.T,X))))
        reg=params.reg*la.norm(beta,'fro')**2
        return loss,reg
    else:
        print('error')
def grad_logreg_multiclass(beta,data,params,numpts):
    n,d=data.X.shape
    if numpts==n:
        X,Y,Z=data.X,data.Y,data.Z
        # remove these 3 lines CC
        #print(X.shape)
        #print(Y.shape)
        #print(beta.shape)
        weight=(np.exp(-np.dot(X,beta)).T/np.sum(np.exp(-np.dot(X,beta)),axis=1)).T
        g_loss=(1./Y.shape[0])*(Z+np.dot(-X.T,weight))
        g_reg=2*params.reg*beta
        return g_loss,g_reg
    elif numpts==1:
        idx=params.idx
        X,Y=data.X[idx,:],data.Y[idx]
        Z=np.zeros(beta.shape)
        Z[:,Y]=X.T
        weight=np.exp(-np.dot(beta.T,X.T))/np.sum(np.exp(-np.dot(beta.T,X.T)))
        g_loss=Z+np.outer(-X,weight)
        g_reg=2*params.reg*beta
        return g_loss,g_reg
    else:
        print('error')
def func_robreg(beta,data,params,numpts):
    n,d=data.X.shape
    if numpts==n:
        X,Y=data.X,data.Y
        loss=la.norm(np.dot(X,beta)-Y)
        reg=0
        return loss,reg        
    elif numpts==1:
        idx=params.idx
        X,Y=data.X[idx,:],data.Y[idx]
        loss=abs(np.dot(X,beta)-Y)
        reg=0
        return loss,reg        
    else:
        print('error')
    
def grad_robreg(beta,data,params,numpts):
    n,d=data.X.shape
    if numpts==n:
        X,Y=data.X,data.Y        
        g_loss=np.dot(X.T,np.sign(np.dot(X,beta)-Y))
        g_reg=0
        return g_loss,g_reg
    elif numpts==1:
        idx=params.idx
        X,Y=data.X[idx,:],data.Y[idx]
        g_loss=X*np.sign(np.dot(X,beta)-Y)
        g_reg=0
        return g_loss,g_reg
    else:
        print('error')

# main
def p1(T=int(1e4),reg=1e-1,R=5.):
    print(T,' iteration and ',reg,' regularization')
    
    # data load and initialize
    X_tr=np.load('A_train.npy')
    y_tr=np.load('b_train.npy')
    X_te=np.load('A_test.npy')
    y_te=np.load('b_test.npy')
    sm,sc=(1/X_tr.shape[0])*la.norm(X_tr,2)**2,(1/X_tr.shape[0])*la.norm(X_tr,-2)**2
    numtr,numte=X_tr.shape[0],X_te.shape[0]
    print(sm,'-smooth and',sc,'-strongly-convex')
    train=Data(X_tr,y_tr)
    test=Data(X_te,y_te)
    
    np.random.seed(1000)
    initbeta=np.random.rand(train.X.shape[1])*10
    var=Variable(initbeta,initbeta,0)
    
    params=Parameter(initbeta,var)
    params.numpts=train.X.shape[0]
    params.reg=reg
    params.max_iter=T
    
    name='p1'+'_reg_'+str(reg)+'_R_'+str(R)+'.eps'
    
    # gd
    params.stepsize=5e-1
    beta_gd,loss_gd,reg_gd,te_error_gd,elapsed_time_gd\
    =descent(gd, func_linreg, grad_linreg,None,\
             train, test, params,fixedstep2)
    loss_gd=[np.sqrt(x*2*numtr) for x in loss_gd]
    te_error_gd=[np.sqrt(x*2*numte) for x in te_error_gd]
    
    #fw
    params.stepsize=2.  
    params.R=R
    beta_fw,loss_fw,reg_fw,te_error_fw,elapsed_time_fw\
    =descent(frankwolfe, func_linreg, grad_linreg,None,\
             train, test, params,fixedstep1)
    loss_fw=[np.sqrt(x*2*numtr) for x in loss_fw]
    te_error_fw=[np.sqrt(x*2*numte) for x in te_error_fw]
    
    #ista
    params.stepsize=1/sm
    beta_ista,loss_ista,reg_ista,te_error_ista,elapsed_time_ista\
    =descent(ista, func_linreg, grad_linreg,None,\
             train, test, params,constantstep)
    loss_ista=[np.sqrt(x*2*numtr) for x in loss_ista]
    te_error_ista=[np.sqrt(x*2*numte) for x in te_error_ista]
    
    #fista
    params.stepsize=1/sm
    beta_fista,loss_fista,reg_fista,te_error_fista,elapsed_time_fista\
    =descent(fista, func_linreg, grad_linreg,None,\
             train, test, params,constantstep)
    loss_fista=[np.sqrt(x*2*numtr) for x in loss_fista]
    te_error_fista=[np.sqrt(x*2*numte) for x in te_error_fista]
    
    
    loss=[loss_gd,loss_fw,loss_ista,loss_fista]
    te_error=[te_error_gd,te_error_fw,te_error_ista,te_error_fista]    
    algo=['GD','FW','ISTA','FISTA']
    marker=['o','+','*','.']
    
    print('min train error is ',min([min(x) for x in loss]))
    print('min test error is',min([min(x) for x in te_error]))

    plotresult(loss,algo,params,marker,title='train error',\
               ylabel=r'$||Ax-b||_2$',xlabel='iteration',\
               yscale='linear',isSave=False,filename='train_'+name,ylim=[0,5e2])
    plotresult(te_error,algo,params,marker,'test error',\
               ylabel=r'$||Ax-b||_2$',xlabel='iteration',\
               yscale='linear',isSave=False,filename='test_'+name,ylim=[0,5e2])
    
    print(la.norm(beta_fista-initbeta,2)/(sm*np.sqrt(T)))

# if __name__ == "__main__":
#     p1(T=int(5e2),reg=1e-5,R=5.)
#     p1(T=int(5e2),reg=1e-3,R=5.)
#     p1(T=int(5e2),reg=1e-1,R=5.)
#     p1(T=int(5e2),reg=1e-0,R=5.)

"""# Optional -- Why we use LASSO

As an optional exercise, you may want to play around with Lasso and explore its properties in the context of machine learning problems. 

To do this: (A) Generate data for yourself at random: create a $n \times d$ matrix $A$ where each entry comes from a standard Gaussian. Choose $d$ much larger than $n$, say, $d = 1000$ and $n = 100$. Now choose the true solution, $x^{\ast}$ to be a $k$-sparse vector. You can do this in many ways. One simple approach is to let $x^{\ast}$ equal 10 on $k=5$ randomly chosen entries, and then zero every where else. Finally, generate $y$ according to
$$
y = Ax + \epsilon,
$$
where $\epsilon$ is zero mean Gaussian noise with variance $0.1$.

Now solve (via an algorithm of your choice) Lasso. Note that you will have to search for a good value for $\lambda$. Compare the solution you get $\hat{x}_{\rm lasso}$ with the true solution, as you vary $\lambda$. You may also want to compare it to the solution when you do not have any regularization.

# Logistic Regression

Logistic regression is a simple statistical classification method which models
the conditional distribution of the class variable $y$ being equal to class $c$
given an input $x \in \mathbb{R}^n$. We will examine two classification tasks, one
classifying newsgroup posts, and the other classifying digits. In these tasks
the input $x$ is some description of the sample (e.g., word counts in the news
case) and $y$ is the category the sample belongs to (e.g., sports, politics).
The Logistic Regression model assumes the class distribution conditioned on $x$
is log-linear:
$$
p(y=c|x,b_{1:C}) = \frac{e^{-b_c^\top x}}{\sum_{j=1}^C e^{-b_j^\top x}},
$$
where $C$ is the total number of classes, and the denominator sums over all
classes to ensure that $p(y|x)$ is a proper probability distribution. Each
class $c \in {1,2, \dots, C}$ has a parameter $b_c$, and $\mathbf{b} \in
\mathbb{R}^{nC}$ is the vector of concatenated parameters $\mathbf{b} =
[b_1^\top,b_2^\top,\dots,b_C^\top]^\top$.  Let $X \in \mathbb{R}^{N \times n}$ be the
data matrix where each sample $x_i^\top$ is a row and $N$ is the number of
samples.  The maximum likelihood approach seeks to find the parameter
$\mathbf{b}$ which maximizes the likelihood of the classes given the input data
and the model:

$$
\max_{b_{1:C}} \; p(y|x,b_{1:C}) = \prod_{i=1}^N p(y_i|x_i,b_{1:C}) = \prod_{i=1}^N \frac{e^{-b_{y_i}^\top x_i}}{\sum_{j=1}^C e^{-b_j^\top x_i}}.
$$

For the purposes of optimization, we can equivalently minimize the negative log
likelihood:
$$
\min_\mathbf{\beta} \ell(\mathbf{\beta}) = -\log p(\textbf{y}|X, \mathbf{\beta}) = \sum_{i=1}^N \left( \beta_{y_i}^\top x_i + \log{\sum_{j=1}^C e^{-\beta_j^\top x_i}} \right).
$$

After optimization, the model can be used to classify a new input by choosing
the class that the model predicts as having the highest likelihood; note that
we don't have to compute the normalizing quantity $\sum_{j=1}^C e^{-b_j^\top x}$
as it is constant across all classes:
$$
y = \arg\max_j p(y=j| x, \mathbf{\beta}) = \arg\min_j \beta_j^\top x
$$
In this problem, you will optimize the logistic regression model for the two
classification tasks mentioned above which vary in dimension and number of
classes. The newsgroup dataset that we consider here has $C=20$. 

We will compare the performance of gradient descent and Nesterov's accelerated gradient
method on the $\ell^2$-regularized version of the logistic regression model:
$$
\min_{\boldsymbol{\beta}} = \frac{1}{N} \sum_{i=1}^N \left( \beta_{y_i}^\top x_i + \log{\sum_{j=1}^C e^{-\beta_j^\top x_i}} \right) + \mu \|\boldsymbol{\beta}\|^2.
$$

Use the training and testing data contained in the four csv files in logistic\_news.zip.

# Part (A) -- Optional -- 
Find the value of $\mu$ that gives you (approximately) the best generalization performance (error on test set). You obtain this by solving the the above optimization problem for different values of $\mu$, and then checking the performance of the solution on the testing set, using the unregularized logistic regression loss. Note that this is not a question about an optimization method.

What value do you get for the test loss after convergence?  

# Part (B) 

If you did Part (A), use the value of $\mu$ you found there. If you did not, use $\mu = 0.001$. 

Plot the loss against iterations for both the test and training data
using the value of $\mu$ from part (a).

# Part (C) 

How do the two algorithms differ in performance, and how does this change
as you decrease $\mu$?
"""

import pandas as pd


def p45(T=int(1e2),reg=1e-1,stepsize=1e-1):
    print(T,' iteration and ',reg, ' reg and ', stepsize,' stepsize')
    # load data and intialize
    train,test=loaddata('./logistic_news.zip')
    np.random.seed(1000)
#     initbeta=np.random.rand(train.Z.shape[0],train.Z.shape[1])*10
    initbeta=np.zeros(train.Z.shape)
    var=Variable(initbeta,initbeta,0)
    
    params=Parameter(initbeta,var)
    params.numpts=train.X.shape[0]
    params.reg=reg    
    params.max_iter=T
    
    name='p4_reg_'+str(reg)+'_step_'+str(stepsize)+'.eps'
    
    
    #gd
    params.stepsize=stepsize
    beta_gd,loss_gd,reg_gd,te_error_gd,elapsed_time_gd\
    =descent(gd, func_logreg_multiclass, grad_logreg_multiclass,None,\
             train, test, params,constantstep)
    #agd
    params.stepsize=stepsize
    beta_agd,loss_agd,reg_agd,te_error_agd,elapsed_time_agd\
    =descent(agd, func_logreg_multiclass, grad_logreg_multiclass,None,\
             train, test, params,constantstep)
    #magd
    params.stepsize=stepsize
    beta_magd,loss_magd,reg_magd,te_error_magd,elapsed_time_magd\
    =descent(magd, func_logreg_multiclass, grad_logreg_multiclass,None,\
             train, test, params,constantstep)
    
    loss=[loss_gd,loss_agd,loss_magd]
    te_error=[te_error_gd,te_error_agd,te_error_magd]
    algo=['GD','AGD','MAGD']
    marker=['+','*','d']
    
    print('min train error is ',min([min(x) for x in loss]))
    print('min test error is',min([min(x) for x in te_error]))

    plotresult(loss,algo,params,marker,title='train error',\
               ylabel='loss',xlabel='iteration',\
               yscale='linear',isSave=False,filename='train_'+name)
    plotresult(te_error,algo,params,marker,title='test error',\
               ylabel='loss',xlabel='iteration',\
               yscale='linear',isSave=True,filename='test_'+name)

if __name__ == "__main__":
    p45(T=int(1000),reg=1e-3,stepsize=1e0)
    # p45(T=int(1e3),reg=1e-3,stepsize=1e-1)
#     p45(T=int(1e3),reg=1e-3,stepsize=1e-3)
#     p45(T=int(1e3),reg=1e-3,stepsize=5e-1)



df_X_te = pd.read_csv('./X_test.csv',header = None)  
df_y_te = pd.read_csv('./y_test.csv', header = None)  
X_te = df_X_te.to_numpy()
y_te = df_y_te.to_numpy()
y_te = y_te.reshape(y_te.shape[1])
df_X_tr = pd.read_csv('./X_train.csv', header = None)  
df_y_tr = pd.read_csv('./y_train.csv', header = None)  
X_tr = df_X_tr.to_numpy()
y_tr = df_y_tr.to_numpy()
y_tr = y_tr.reshape(y_tr.shape[1])
Z_tr,Z_te=[],[]
for j in range(len(np.unique(y_tr))):
    Z_tr.append(np.sum(X_tr[np.where(y_tr==j)[0],:],axis=0))
    Z_te.append(np.sum(X_te[np.where(y_te==j)[0],:],axis=0))
Z_tr=np.asarray(Z_tr).T
Z_te=np.asarray(Z_te).T
train= Data(X_tr,y_tr)
train.Z=Z_tr
test = Data(X_te,y_te)
test.Z=Z_te
initbeta=np.zeros(train.Z.shape)
var=Variable(initbeta,initbeta,0)
params=Parameter(initbeta,var)
params.numpts=train.X.shape[0]
reg = 1e-3
T = int(1e3)
stepsize = 1e0
params.reg=reg    
params.max_iter=T
params.stepsize=stepsize

np.unique(y_te)

Z_tr.shape

X_tr.shape

"""# Problem Set 5 Solutions

An Extra MD Problem using a newsgroup data set.
"""

def ProbSet5_MD_Extra(T=int(1e3),stepsize=1e-2):
    print(T,' iteration')
    # data load
    vectorizer= TfidfVectorizer()
    categories = ['alt.atheism']
    news_train = fetch_20newsgroups(subset='train',categories=categories)
    W_train = vectorizer.fit_transform(news_train.data).T
    label_train = news_train.target

    news_test = fetch_20newsgroups(subset='test',categories=categories)
    W_test = vectorizer.transform(news_test.data).T
    label_test = news_test.target
    y=W_test[:,0]
    
    #NMF
    num_topic=10
    model=NMF(n_components=num_topic, init='random', random_state=0)
    X=model.fit_transform(W_train)
    X=np.asarray(X)
    y=y.toarray().T[0]
    
    train=Data(X,y)
    dummy=Data(X,y)
    sm,sc=(1/train.X.shape[0])*la.norm(train.X,2)**2,(1/train.X.shape[0])*la.norm(train.X,-2)**2
    
    np.random.seed(1000)
    initbeta=np.random.rand(train.X.shape[1])   
#     initbeta=np.zeros(train.X.shape[1])
    var=Variable(initbeta,initbeta,0)
    
    params=Parameter(initbeta,var)
    params.numpts=train.X.shape[0]
    params.reg=0.
    params.max_iter=T
    
    name='p6_'+str(stepsize)+'.eps'
    
    
    #projected gradient
    params.stepsize=stepsize
    beta_gd,loss_gd,reg_gd,te_error_gd,elapsed_time_gd\
    =descent(pgd, func_robreg,grad_robreg,None,\
             train, dummy, params,fixedstep1)
    print('topic distribution from projected gradient descent is ')
    print(beta_gd)
    
    #mirror descent
    params.stepsize=stepsize
    beta_md,loss_md,reg_md,te_error_md,elapsed_time_md\
    =descent(mirrordescent, func_robreg,grad_robreg,None,\
             train, dummy, params,fixedstep1)
    print('topic distribution from mirror descent is ')
    print(beta_md)
    
    loss=[loss_gd,loss_md]
    te_error=[te_error_gd,te_error_md]
    algo=['PGD','MD']
    marker=['+','*']
    
    print('min train error is ',min([min(x) for x in loss]))
    
    plotresult(loss,algo,params,marker,title='train error',\
               ylabel='loss',xlabel='iteration',\
               yscale='linear',isSave=False,filename='train_'+name)

    plotresult(te_error,algo,params,marker,title='test error',\
               ylabel='loss',xlabel='iteration',\
               yscale='linear',isSave=False,filename='test_'+name)

if __name__ == "__main__":
    ProbSet5_MD_Extra(T=int(1e4),stepsize=1e-2)

"""And now the problem that was on the homework"""

def ProbSet5_MD(T=int(1e3),stepsize=1e-2):
    print(T,' iteration')
    X = np.load("X.npy")
    y = np.load("y.npy")
    
    train=Data(X,y)
    dummy=Data(X,y)
    sm,sc=(1/train.X.shape[0])*la.norm(train.X,2)**2,(1/train.X.shape[0])*la.norm(train.X,-2)**2
    
    np.random.seed(1000)
    initbeta=np.random.rand(train.X.shape[1])   
#     initbeta=np.zeros(train.X.shape[1])
    var=Variable(initbeta,initbeta,0)
    
    params=Parameter(initbeta,var)
    params.numpts=train.X.shape[0]
    params.reg=0.
    params.max_iter=T
    
    name='p6_'+str(stepsize)+'.eps'
    
    
    #projected gradient
    params.stepsize=stepsize
    beta_gd,loss_gd,reg_gd,te_error_gd,elapsed_time_gd=descent(gd, func_robreg,grad_robreg,None,train, dummy, params,fixedstep1)
    print('topic distribution from projected gradient descent is ')
    print(beta_gd)
    
    #mirror descent
    params.stepsize=stepsize
    beta_md,loss_md,reg_md,te_error_md,elapsed_time_md=descent(mirrordescent, func_robreg,grad_robreg,None,train, dummy, params,fixedstep1)
    print('topic distribution from mirror descent is ')
    print(beta_md)
    loss=[loss_gd,loss_md]
    te_error=[te_error_gd,te_error_md]
    algo=['GD','MD']
    marker=['+','*']
    
    print('min train error is ',min([min(x) for x in loss]))
    
    plotresult(loss,algo,params,marker,title='train error',ylabel='loss',xlabel='iteration',yscale='linear',isSave=False,filename='train_'+name)

    plotresult(te_error,algo,params,marker,title='test error',ylabel='loss',xlabel='iteration',yscale='linear',isSave=False,filename='test_'+name)

if __name__ == "__main__":
    ProbSet5_MD(T=int(1e2),stepsize=1e-1)

